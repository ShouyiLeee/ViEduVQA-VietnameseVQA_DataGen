{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-19T09:54:36.818793Z",
     "iopub.status.busy": "2025-06-19T09:54:36.818578Z",
     "iopub.status.idle": "2025-06-19T09:56:04.119142Z",
     "shell.execute_reply": "2025-06-19T09:56:04.118043Z",
     "shell.execute_reply.started": "2025-06-19T09:54:36.818775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.46.0 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch torchvision accelerate peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T09:56:04.121418Z",
     "iopub.status.busy": "2025-06-19T09:56:04.121169Z",
     "iopub.status.idle": "2025-06-19T09:56:04.544045Z",
     "shell.execute_reply": "2025-06-19T09:56:04.543264Z",
     "shell.execute_reply.started": "2025-06-19T09:56:04.121394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39391e1e3f7d4f159eb788890201aa67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:05:17.797400Z",
     "iopub.status.busy": "2025-06-19T10:05:17.796620Z",
     "iopub.status.idle": "2025-06-19T10:05:17.801944Z",
     "shell.execute_reply": "2025-06-19T10:05:17.801201Z",
     "shell.execute_reply.started": "2025-06-19T10:05:17.797373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    PaliGemmaProcessor, \n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training, get_peft_config\n",
    "from datasets import Dataset, load_dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:05:18.437995Z",
     "iopub.status.busy": "2025-06-19T10:05:18.437410Z",
     "iopub.status.idle": "2025-06-19T10:05:41.213903Z",
     "shell.execute_reply": "2025-06-19T10:05:41.213291Z",
     "shell.execute_reply.started": "2025-06-19T10:05:18.437972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model to GPU...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fd7cfd8e564ce087d22300f87b2cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded on: cuda:0\n",
      "🧠 GPU memory after model load: 6.97GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Cấu hình 4-bit quantization - Force GPU usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 2. Tải model và processor - Explicit GPU placement\n",
    "model_id = \"google/paligemma-3b-pt-224\"\n",
    "processor = PaliGemmaProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"📥 Loading model to GPU...\")\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # This should put model on GPU\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Verify model is on GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ Model loaded on: {next(model.parameters()).device}\")\n",
    "    print(f\"🧠 GPU memory after model load: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "else:\n",
    "    print(\"⚠️ Model loaded on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:05:41.215189Z",
     "iopub.status.busy": "2025-06-19T10:05:41.214957Z",
     "iopub.status.idle": "2025-06-19T10:05:41.716027Z",
     "shell.execute_reply": "2025-06-19T10:05:41.715106Z",
     "shell.execute_reply.started": "2025-06-19T10:05:41.215170Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22,597,632 || all params: 2,946,064,112 || trainable%: 0.7670\n"
     ]
    }
   ],
   "source": [
    "# 3. Cấu hình LoRA - Tối ưu cho tốc độ\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,  # Reduced from 16 to 8 for faster training\n",
    "    lora_alpha=16,  # Reduced proportionally\n",
    "    lora_dropout=0.05,  # Reduced dropout\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Áp dụng LoRA lên model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:07:27.747720Z",
     "iopub.status.busy": "2025-06-19T10:07:27.747398Z",
     "iopub.status.idle": "2025-06-19T10:15:37.577714Z",
     "shell.execute_reply": "2025-06-19T10:15:37.576703Z",
     "shell.execute_reply.started": "2025-06-19T10:07:27.747696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mẫu: 18838\n",
      "Dataset có 18838 mẫu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def load_data_optimized():\n",
    "    csv_file = \"/kaggle/input/vieduvqa/Verify_Convert_80.csv\"\n",
    "    image_folder = \"/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu\"\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    # df = df.head(100).copy()  # Increased sample size for better training\n",
    "    print(f\"Số mẫu: {len(df)}\")\n",
    "    \n",
    "    # Pre-validate images to avoid runtime errors\n",
    "    valid_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = row['ImageID']\n",
    "        category = image_id.split('_')[0]\n",
    "        image_path = os.path.join(image_folder, category, f\"{image_id}.png\")\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                # Pre-load and validate image\n",
    "                img = Image.open(image_path).convert('RGB')\n",
    "                # Resize to consistent size for faster processing\n",
    "                img = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "                \n",
    "                valid_data.append({\n",
    "                    'image': img,\n",
    "                    'question': row['Question'],\n",
    "                    'answer': row['Answer']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi ảnh {image_id}: {e}\")\n",
    "        else:\n",
    "            print(f\"Không tìm thấy: {image_path}\")\n",
    "    \n",
    "    return Dataset.from_list(valid_data)\n",
    "\n",
    "# Load data\n",
    "dataset = load_data_optimized()\n",
    "print(f\"Dataset có {len(dataset)} mẫu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:15:37.579408Z",
     "iopub.status.busy": "2025-06-19T10:15:37.579183Z",
     "iopub.status.idle": "2025-06-19T10:15:37.599854Z",
     "shell.execute_reply": "2025-06-19T10:15:37.599051Z",
     "shell.execute_reply.started": "2025-06-19T10:15:37.579389Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 16954, Test: 1884\n"
     ]
    }
   ],
   "source": [
    "# Chia train/test\n",
    "train_test = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test['train']\n",
    "test_dataset = train_test['test']\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:15:37.600831Z",
     "iopub.status.busy": "2025-06-19T10:15:37.600608Z",
     "iopub.status.idle": "2025-06-19T10:15:37.618292Z",
     "shell.execute_reply": "2025-06-19T10:15:37.617682Z",
     "shell.execute_reply.started": "2025-06-19T10:15:37.600813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=224x224 at 0x782493CA4490>, 'question': 'Ngoài cậu bé, những đồ vật nào khác có màu đỏ trong ảnh?', 'answer': 'Ngoài cậu bé, những đồ vật khác có màu đỏ trong ảnh là quả táo, một số cuốn sách, một phần của chiếc ghế và một hộp Rubik.\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T09:29:01.194758Z",
     "iopub.status.busy": "2025-06-19T09:29:01.193934Z",
     "iopub.status.idle": "2025-06-19T09:29:01.201325Z",
     "shell.execute_reply": "2025-06-19T09:29:01.200578Z",
     "shell.execute_reply.started": "2025-06-19T09:29:01.194732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def preprocess_batch(examples):\n",
    "#     \"\"\"Optimized preprocessing for batch processing\"\"\"\n",
    "#     batch_size = len(examples['image'])\n",
    "    \n",
    "#     # Process all images and questions at once\n",
    "#     images = []\n",
    "#     questions = []\n",
    "#     answers = []\n",
    "    \n",
    "#     for i in range(batch_size):\n",
    "#         image = examples['image'][i]\n",
    "#         question = f\"<image>{examples['question'][i]}\"\n",
    "#         answer = examples['answer'][i]\n",
    "        \n",
    "#         images.append(image)\n",
    "#         questions.append(question)\n",
    "#         answers.append(answer)\n",
    "    \n",
    "#     # Batch tokenization for efficiency\n",
    "#     inputs = processor(\n",
    "#         text=questions,\n",
    "#         images=images,\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=256,  # Reduced from 512\n",
    "#         do_resize=True,\n",
    "#         size={\"height\": 224, \"width\": 224}\n",
    "#     )\n",
    "    \n",
    "#     # Tokenize labels\n",
    "#     with processor.tokenizer.as_target_tokenizer():\n",
    "#         labels = processor.tokenizer(\n",
    "#             text=answers,\n",
    "#             return_tensors=\"pt\",\n",
    "#             truncation=True,\n",
    "#             padding=True,\n",
    "#             max_length=64  # Reduced from 128\n",
    "#         )\n",
    "    \n",
    "#     inputs[\"labels\"] = labels.input_ids\n",
    "#     return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:19:03.153454Z",
     "iopub.status.busy": "2025-06-19T10:19:03.152620Z",
     "iopub.status.idle": "2025-06-19T10:19:03.159554Z",
     "shell.execute_reply": "2025-06-19T10:19:03.158717Z",
     "shell.execute_reply.started": "2025-06-19T10:19:03.153403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(examples):\n",
    "    \"\"\"Chuẩn hóa batch dữ liệu cho VQA\"\"\"\n",
    "    images = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for image, question, answer in zip(examples[\"image\"], examples[\"question\"], examples[\"answer\"]):\n",
    "        questions.append(f\"<image>{question}\")\n",
    "        images.append(image)\n",
    "        answers.append(answer)\n",
    "\n",
    "    # Tokenize input (image + question)\n",
    "    inputs = processor(\n",
    "        text=questions,\n",
    "        images=images,\n",
    "        return_tensors=None,       # ❗ KHÔNG ép thành tensor ở đây\n",
    "        padding=False,             # ❗ KHÔNG padding\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        do_resize=True,\n",
    "        size={\"height\": 224, \"width\": 224}\n",
    "    )\n",
    "\n",
    "    # Tokenize labels (answer)\n",
    "    with processor.tokenizer.as_target_tokenizer():\n",
    "        labels = processor.tokenizer(\n",
    "            text=answers,\n",
    "            return_tensors=None,    # ❗ Giữ raw list of input_ids\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=64\n",
    "        )\n",
    "\n",
    "    # Để labels dạng list[int] (không tensor), để collator xử lý\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:19:05.813202Z",
     "iopub.status.busy": "2025-06-19T10:19:05.812386Z",
     "iopub.status.idle": "2025-06-19T10:21:41.459366Z",
     "shell.execute_reply": "2025-06-19T10:21:41.458486Z",
     "shell.execute_reply.started": "2025-06-19T10:19:05.813167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Preprocessing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627580b13ec4423695aa4be900485097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train data:   0%|          | 0/16954 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52946847212b46e4a46f90e48eaa90c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test data:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d1d4dc89c34976b2703e2d0efbe507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/21 shards):   0%|          | 0/16954 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82d2880c49c4157930b202bf8083a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Data preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "print(\"🔄 Preprocessing data...\")\n",
    "train_processed = train_dataset.map(\n",
    "    preprocess_batch, \n",
    "    batched=True, \n",
    "    batch_size=4,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1,\n",
    "    desc=\"Processing train data\"\n",
    ")\n",
    "\n",
    "test_processed = test_dataset.map(\n",
    "    preprocess_batch, \n",
    "    batched=True, \n",
    "    batch_size=4,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1,\n",
    "    desc=\"Processing test data\"\n",
    ")\n",
    "# Move processed data to GPU if needed (for small datasets)\n",
    "\n",
    "# train_processed.save_to_disk(\"dataset_cached/train\")\n",
    "# test_processed.save_to_disk(\"dataset_cached/test\")\n",
    "print(\"💾 Data preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:56:10.814240Z",
     "iopub.status.busy": "2025-06-19T07:56:10.813665Z",
     "iopub.status.idle": "2025-06-19T07:56:10.825833Z",
     "shell.execute_reply": "2025-06-19T07:56:10.825218Z",
     "shell.execute_reply.started": "2025-06-19T07:56:10.814213Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing OK!\n"
     ]
    }
   ],
   "source": [
    "# # Test 1 sample trước\n",
    "# sample = dataset[0]  # 1 sample đầu tiên\n",
    "# prompt = f\"answer vi\\n{sample['question']}\"\n",
    "# inputs = processor(images=sample[\"image\"], text=prompt, return_tensors=\"pt\")\n",
    "# print(\"Preprocessing OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:24:12.206006Z",
     "iopub.status.busy": "2025-06-19T10:24:12.205352Z",
     "iopub.status.idle": "2025-06-19T10:24:12.210611Z",
     "shell.execute_reply": "2025-06-19T10:24:12.209904Z",
     "shell.execute_reply.started": "2025-06-19T10:24:12.205982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'pixel_values', 'labels'],\n",
       "    num_rows: 16954\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:32:30.693090Z",
     "iopub.status.busy": "2025-06-19T10:32:30.692484Z",
     "iopub.status.idle": "2025-06-19T10:32:30.720414Z",
     "shell.execute_reply": "2025-06-19T10:32:30.719897Z",
     "shell.execute_reply.started": "2025-06-19T10:32:30.693068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6. GPU-Optimized Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pali_gemma_vqa_8bit\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,  # Khi dùng 4-bit, tính toán bên trong đã là bfloat16\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=[],\n",
    "    save_total_limit=1,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:32:33.343670Z",
     "iopub.status.busy": "2025-06-19T10:32:33.343016Z",
     "iopub.status.idle": "2025-06-19T10:32:33.347332Z",
     "shell.execute_reply": "2025-06-19T10:32:33.346507Z",
     "shell.execute_reply.started": "2025-06-19T10:32:33.343645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:32:33.887478Z",
     "iopub.status.busy": "2025-06-19T10:32:33.887151Z",
     "iopub.status.idle": "2025-06-19T10:32:33.891778Z",
     "shell.execute_reply": "2025-06-19T10:32:33.891073Z",
     "shell.execute_reply.started": "2025-06-19T10:32:33.887426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class TerminateOnEndCallback(TrainerCallback):\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        print(\"🚩 Training ended.\")\n",
    "        exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T09:32:26.834314Z",
     "iopub.status.busy": "2025-06-19T09:32:26.833620Z",
     "iopub.status.idle": "2025-06-19T09:32:26.839792Z",
     "shell.execute_reply": "2025-06-19T09:32:26.838986Z",
     "shell.execute_reply.started": "2025-06-19T09:32:26.834293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# class OptimizedVQADataCollator:\n",
    "#     def __init__(self, processor):\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __call__(self, features):\n",
    "#         batch = {}\n",
    "#         for key in features[0].keys():\n",
    "#             vals = [f[key] for f in features]\n",
    "#             if key == \"labels\":\n",
    "#                 # đảm bảo từng phần tử là tensor\n",
    "#                 tensors = [v if isinstance(v, torch.Tensor) else torch.tensor(v) for v in vals]\n",
    "#                 batch[key] = pad_sequence(tensors, batch_first=True, padding_value=-100)\n",
    "#             else:\n",
    "#                 tensors = [v if isinstance(v, torch.Tensor) else torch.tensor(v) for v in vals]\n",
    "#                 batch[key] = torch.stack(tensors)\n",
    "#         return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:32:35.639213Z",
     "iopub.status.busy": "2025-06-19T10:32:35.638948Z",
     "iopub.status.idle": "2025-06-19T10:32:35.646352Z",
     "shell.execute_reply": "2025-06-19T10:32:35.645613Z",
     "shell.execute_reply.started": "2025-06-19T10:32:35.639194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 8. GPU Performance Monitoring Callback\n",
    "class GPUOptimizedTrainingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        import time\n",
    "        self.start_time = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"🚀 Training started on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"💾 Initial GPU memory: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "        else:\n",
    "            print(\"⚠️ Training started on CPU - this will be very slow!\")\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 10 == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                gpu_util = f\"{gpu_memory:.2f}GB/{gpu_reserved:.2f}GB\"\n",
    "                print(f\"Step {state.global_step} | GPU Memory: {gpu_util}\")\n",
    "                \n",
    "                # Check for GPU utilization issues\n",
    "                if gpu_memory < 1.0:  # Less than 1GB usage might indicate CPU training\n",
    "                    print(\"⚠️ Low GPU memory usage - check if model is on GPU!\")\n",
    "            else:\n",
    "                print(f\"Step {state.global_step} | Running on CPU\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"train_loss\" in logs:\n",
    "            import time\n",
    "            elapsed = time.time() - self.start_time if self.start_time else 0\n",
    "            steps_per_sec = state.global_step / elapsed if elapsed > 0 else 0\n",
    "            device_info = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "            print(f\"Loss: {logs['train_loss']:.4f} | Steps/sec: {steps_per_sec:.2f} | Device: {device_info}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:32:37.706121Z",
     "iopub.status.busy": "2025-06-19T10:32:37.705488Z",
     "iopub.status.idle": "2025-06-19T10:32:37.714219Z",
     "shell.execute_reply": "2025-06-19T10:32:37.713469Z",
     "shell.execute_reply.started": "2025-06-19T10:32:37.706097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class OptimizedVQADataCollator:\n",
    "    def __init__(self, processor, label_pad_token_id=-100):\n",
    "        self.processor = processor\n",
    "        self.label_pad_token_id = label_pad_token_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = {}\n",
    "\n",
    "        # Pad input_ids, attention_mask theo max_length trong batch\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
    "        \n",
    "        # Nếu pixel_values là list thì convert\n",
    "        pixel_values = torch.stack([\n",
    "            torch.tensor(f[\"pixel_values\"]) if not isinstance(f[\"pixel_values\"], torch.Tensor) else f[\"pixel_values\"]\n",
    "            for f in features\n",
    "        ])\n",
    "\n",
    "        input_ids_padded = pad_sequence(\n",
    "            [torch.tensor(ids) for ids in input_ids],\n",
    "            batch_first=True,\n",
    "            padding_value=self.processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        attention_mask_padded = pad_sequence(\n",
    "            [torch.tensor(mask) for mask in attention_mask],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "\n",
    "        # Pad labels theo input_ids length\n",
    "        label_pad_len = input_ids_padded.shape[1]\n",
    "        labels = [torch.tensor(f[\"labels\"]) for f in features]\n",
    "        labels_padded = pad_sequence(labels, batch_first=True, padding_value=self.label_pad_token_id)\n",
    "\n",
    "        # Align labels to input_ids shape\n",
    "        if labels_padded.shape[1] < label_pad_len:\n",
    "            pad_len = label_pad_len - labels_padded.shape[1]\n",
    "            labels_padded = torch.nn.functional.pad(labels_padded, (0, pad_len), value=self.label_pad_token_id)\n",
    "        elif labels_padded.shape[1] > label_pad_len:\n",
    "            labels_padded = labels_padded[:, :label_pad_len]\n",
    "\n",
    "        batch[\"input_ids\"] = input_ids_padded\n",
    "        batch[\"attention_mask\"] = attention_mask_padded\n",
    "        batch[\"pixel_values\"] = pixel_values\n",
    "        batch[\"labels\"] = labels_padded\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:32:39.093052Z",
     "iopub.status.busy": "2025-06-19T10:32:39.092761Z",
     "iopub.status.idle": "2025-06-19T10:32:39.261631Z",
     "shell.execute_reply": "2025-06-19T10:32:39.260943Z",
     "shell.execute_reply.started": "2025-06-19T10:32:39.093019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([2, 273])\n",
      "attention_mask: torch.Size([2, 273])\n",
      "pixel_values: torch.Size([2, 3, 224, 224])\n",
      "labels: torch.Size([2, 273])\n"
     ]
    }
   ],
   "source": [
    "data_collator = OptimizedVQADataCollator(processor)\n",
    "\n",
    "batch = data_collator([train_processed[i] for i in range(2)])\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    else:\n",
    "        print(f\"{k}: {type(v)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-19T10:32:39.881831Z",
     "iopub.status.busy": "2025-06-19T10:32:39.881556Z",
     "iopub.status.idle": "2025-06-19T18:14:42.661770Z",
     "shell.execute_reply": "2025-06-19T18:14:42.660856Z",
     "shell.execute_reply.started": "2025-06-19T10:32:39.881809Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/933712960.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Bắt đầu fine-tuning PaliGemma...\n",
      "📊 Training với 16954 samples\n",
      "🖥️  Device: cuda:0\n",
      "🚀 Training started on GPU: Tesla P100-PCIE-16GB\n",
      "💾 Initial GPU memory: 8.13GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1058' max='1058' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1058/1058 7:41:27, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>38.632900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>33.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>32.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>32.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>32.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>32.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>30.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>29.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>30.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>31.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>30.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>29.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>30.766600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>29.762900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>30.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>29.473400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>29.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>29.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>29.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>30.728800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>29.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>29.914400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>28.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>29.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>28.891400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>29.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>28.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>28.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>28.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>29.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>27.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>29.851100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>29.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>29.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>28.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>28.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>28.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>29.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>26.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>27.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>29.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>28.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>28.760700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>28.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>27.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>27.959400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>28.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>27.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>28.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>27.962300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>27.901200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>28.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>27.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>27.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>26.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>24.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>25.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>26.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>27.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>25.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>25.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>26.670600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>27.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>26.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>26.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>27.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>25.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>26.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>26.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>25.773600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>25.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>25.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>26.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>24.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>25.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>26.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>25.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>26.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>27.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>25.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>25.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>26.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>26.779600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>25.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>26.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>26.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>26.267200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>25.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>26.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>24.497800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>26.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>25.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>23.887600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>26.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>26.537800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>25.348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>24.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>26.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>24.922200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>25.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>24.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>26.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>24.833800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>26.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>26.534800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | GPU Memory: 8.31GB/16.62GB\n",
      "Step 20 | GPU Memory: 8.31GB/16.62GB\n",
      "Step 30 | GPU Memory: 8.31GB/16.62GB\n",
      "Step 40 | GPU Memory: 8.31GB/16.62GB\n",
      "Step 50 | GPU Memory: 8.31GB/16.65GB\n",
      "Step 60 | GPU Memory: 8.31GB/16.68GB\n",
      "Step 70 | GPU Memory: 8.31GB/16.68GB\n",
      "Step 80 | GPU Memory: 8.31GB/16.68GB\n",
      "Step 90 | GPU Memory: 8.31GB/15.56GB\n",
      "Step 100 | GPU Memory: 8.31GB/14.45GB\n",
      "Step 110 | GPU Memory: 8.31GB/14.46GB\n",
      "Step 120 | GPU Memory: 8.31GB/14.46GB\n",
      "Step 130 | GPU Memory: 8.31GB/14.46GB\n",
      "Step 140 | GPU Memory: 8.31GB/14.46GB\n",
      "Step 150 | GPU Memory: 8.31GB/14.46GB\n",
      "Step 160 | GPU Memory: 8.31GB/14.46GB\n",
      "Step 170 | GPU Memory: 8.31GB/14.46GB\n",
      "Step 180 | GPU Memory: 8.31GB/16.69GB\n",
      "Step 190 | GPU Memory: 8.31GB/16.69GB\n",
      "Step 200 | GPU Memory: 8.31GB/16.69GB\n",
      "Step 210 | GPU Memory: 8.31GB/16.69GB\n",
      "Step 220 | GPU Memory: 8.31GB/16.69GB\n",
      "Step 230 | GPU Memory: 8.31GB/14.50GB\n",
      "Step 240 | GPU Memory: 8.31GB/15.62GB\n",
      "Step 250 | GPU Memory: 8.31GB/15.62GB\n",
      "Step 260 | GPU Memory: 8.31GB/14.51GB\n",
      "Step 270 | GPU Memory: 8.31GB/14.51GB\n",
      "Step 280 | GPU Memory: 8.31GB/14.51GB\n",
      "Step 290 | GPU Memory: 8.31GB/14.51GB\n",
      "Step 300 | GPU Memory: 8.31GB/14.51GB\n",
      "Step 310 | GPU Memory: 8.31GB/14.51GB\n",
      "Step 320 | GPU Memory: 8.31GB/14.51GB\n",
      "Step 330 | GPU Memory: 8.31GB/14.51GB\n",
      "Step 340 | GPU Memory: 8.31GB/14.51GB\n",
      "Step 350 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 360 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 370 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 380 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 390 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 400 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 410 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 420 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 430 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 440 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 450 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 460 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 470 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 480 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 490 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 500 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 510 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 520 | GPU Memory: 8.31GB/15.64GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 540 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 550 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 560 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 570 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 580 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 590 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 600 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 610 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 620 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 630 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 640 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 650 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 660 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 670 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 680 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 690 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 700 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 710 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 720 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 730 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 740 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 750 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 760 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 770 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 780 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 790 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 800 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 810 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 820 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 830 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 840 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 850 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 860 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 870 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 880 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 890 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 900 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 910 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 920 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 930 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 940 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 950 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 960 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 970 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 980 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 990 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 1000 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 1010 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 1020 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 1030 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 1040 | GPU Memory: 8.31GB/15.64GB\n",
      "Step 1050 | GPU Memory: 8.31GB/15.64GB\n",
      "Loss: 27.7179 | Steps/sec: 0.04 | Device: GPU\n",
      "💾 Đang lưu model...\n",
      "✅ Hoàn thành!\n"
     ]
    }
   ],
   "source": [
    "# # 9. Initialize Trainer with GPU verification\n",
    "# data_collator = OptimizedVQADataCollator(processor)\n",
    "\n",
    "\n",
    "# Shape phải giống nhau ở dim1\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_processed,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    callbacks=[GPUOptimizedTrainingCallback()]\n",
    ")\n",
    "\n",
    "print(\"🚀 Bắt đầu fine-tuning PaliGemma...\")\n",
    "print(f\"📊 Training với {len(train_dataset)} samples\")\n",
    "print(f\"🖥️  Device: {training_args.device}\")\n",
    "\n",
    "# Bắt đầu training\n",
    "try:\n",
    "    trainer.train()\n",
    "    \n",
    "    # Lưu model\n",
    "    print(\"💾 Đang lưu model...\")\n",
    "    trainer.save_model(\"./paligemma-vietnamese-vqa-final\")\n",
    "    processor.save_pretrained(\"./paligemma-vietnamese-vqa-final\")\n",
    "    print(\"✅ Hoàn thành!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"⚠️ Training bị dừng bởi người dùng\")\n",
    "    trainer.save_model(\"./paligemma-vietnamese-vqa-interrupted\")\n",
    "    processor.save_pretrained(\"./paligemma-vietnamese-vqa-interrupted\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T18:20:16.144568Z",
     "iopub.status.busy": "2025-06-19T18:20:16.143944Z",
     "iopub.status.idle": "2025-06-19T18:20:25.388627Z",
     "shell.execute_reply": "2025-06-19T18:20:25.387914Z",
     "shell.execute_reply.started": "2025-06-19T18:20:16.144541Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu hỏi: Cô giáo mặc trang phục màu gì?\n",
      "Câu trả lời: màu xanh lá.\n",
      " xanh.\n",
      " xanh.\n",
      " xanh\n",
      " xanh lá.\n",
      " xanh lá\n",
      " xanh,\n",
      " xanh\n",
      " xanh.\n",
      " xanh lá.\n",
      " xanh.\n",
      " xanh\n",
      " xanh.\n",
      " xanh lá\n",
      " xanh.\n",
      " xanh\n",
      " xanh\n",
      " xanh lá xanh\n",
      " xanh,\n",
      " xanh\n",
      " xanh,\n",
      " xanh,\n",
      " xanh\n",
      " xanh lá\n",
      " xanh xanh\n",
      " xanh xanh\n",
      " xanh xanh\n",
      " xanh lá\n",
      " xanh.\n",
      " xanh.\n",
      " xanh.\n",
      " xanh\n",
      " xanh\n",
      " xanh. xanh\n",
      " xanh\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(image_path, question, model, processor):\n",
    "    \"\"\"Sinh câu trả lời cho câu hỏi về ảnh\"\"\"\n",
    "    \n",
    "    # Tải và xử lý ảnh\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Chuẩn bị input\n",
    "    prompt = f\"<image>{question}\"\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = processor.tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Test model\n",
    "test_image = \"/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000001.png\"\n",
    "test_question = \"Cô giáo mặc trang phục màu gì?\"\n",
    "\n",
    "answer = generate_answer(test_image, test_question, model, processor)\n",
    "print(f\"Câu hỏi: {test_question}\")\n",
    "print(f\"Câu trả lời: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-18T20:07:30.049769Z",
     "iopub.status.busy": "2025-06-18T20:07:30.049201Z",
     "iopub.status.idle": "2025-06-18T20:07:33.627355Z",
     "shell.execute_reply": "2025-06-18T20:07:33.626373Z",
     "shell.execute_reply.started": "2025-06-18T20:07:30.049745Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/1256526511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m model = PaliGemmaForConditionalGeneration.from_pretrained(\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "# # device = \"cuda\"\n",
    "\n",
    "# import torch\n",
    "# from transformers import (\n",
    "#     PaliGemmaProcessor, \n",
    "#     PaliGemmaForConditionalGeneration,\n",
    "#     BitsAndBytesConfig,\n",
    "#     TrainingArguments,\n",
    "#     Trainer\n",
    "# )\n",
    "# from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# # Cấu hình 8-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,\n",
    "#     bnb_8bit_use_double_quant=True,\n",
    "#     bnb_8bit_quant_type=\"nf4\",\n",
    "#     bnb_8bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# # Tải model và processor\n",
    "# model_id = \"google/paligemma-3b-pt-224\"\n",
    "\n",
    "# processor = PaliGemmaProcessor.from_pretrained(\n",
    "#     model_id,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "#     model_id,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# # Cấu hình LoRA\n",
    "# lora_config = LoraConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM,\n",
    "#     inference_mode=False,\n",
    "#     r=8,  # Rank của LoRA\n",
    "#     lora_alpha=16,  # Alpha parameter\n",
    "#     lora_dropout=0.1,\n",
    "#     target_modules=[\n",
    "#         \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "#         \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "#     ],\n",
    "#     bias=\"none\"\n",
    "# )\n",
    "\n",
    "# # Áp dụng LoRA lên model\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T20:07:12.200695Z",
     "iopub.status.busy": "2025-06-18T20:07:12.200419Z",
     "iopub.status.idle": "2025-06-18T20:07:12.206206Z",
     "shell.execute_reply": "2025-06-18T20:07:12.204985Z",
     "shell.execute_reply.started": "2025-06-18T20:07:12.200677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded on: cuda:0\n",
      "🧠 GPU memory after model load: 7.26GB\n"
     ]
    }
   ],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     print(f\"✅ Model loaded on: {next(model.parameters()).device}\")\n",
    "#     print(f\"🧠 GPU memory after model load: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "# else:\n",
    "#     print(\"⚠️ Model loaded on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:44:26.292157Z",
     "iopub.status.busy": "2025-06-18T18:44:26.291534Z",
     "iopub.status.idle": "2025-06-18T18:44:39.587771Z",
     "shell.execute_reply": "2025-06-18T18:44:39.587192Z",
     "shell.execute_reply.started": "2025-06-18T18:44:26.292137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from datasets import Dataset\n",
    "\n",
    "# df = pd.read_csv(\"/kaggle/input/vieduvqa/Verify_Convert_80.csv\")\n",
    "# # Function tìm đường dẫn\n",
    "# def get_image_path(row):\n",
    "#     sub = row[\"ImageID\"].split(\"_\", 1)[0]  # lấy subfolder: Education, Life, ...\n",
    "#     fname = row[\"ImageID\"] + \".png\"       # nếu ảnh là .jpg\n",
    "#     path = os.path.join(\"/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu\", sub, fname)\n",
    "#     if os.path.exists(path):\n",
    "#         return path\n",
    "#     else:\n",
    "#         raise FileNotFoundError(f\"Không tìm thấy: {path}\")\n",
    "\n",
    "# df[\"image_path\"] = df.apply(get_image_path, axis=1)\n",
    "# dataset = Dataset.from_pandas(df[[\"image_path\", \"Question\", \"Answer\"]])\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T20:04:26.614197Z",
     "iopub.status.busy": "2025-06-18T20:04:26.613351Z",
     "iopub.status.idle": "2025-06-18T20:04:27.310953Z",
     "shell.execute_reply": "2025-06-18T20:04:27.310198Z",
     "shell.execute_reply.started": "2025-06-18T20:04:26.614164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mẫu: 10\n",
      "Dataset có 10 mẫu\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# from datasets import Dataset\n",
    "\n",
    "# # Đường dẫn\n",
    "# csv_file = \"/kaggle/input/vieduvqa/Verify_Convert_80.csv\"  # Thay tên file CSV của bạn\n",
    "# image_folder = \"/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu\"      # Folder chứa Education, Nature, Life, Object, Others\n",
    "\n",
    "# # Đọc CSV\n",
    "# df = pd.read_csv(csv_file)\n",
    "# df = df.head(10).copy()\n",
    "\n",
    "# print(f\"Số mẫu: {len(df)}\")\n",
    "\n",
    "# # Chuyển đổi\n",
    "# data = []\n",
    "# for _, row in df.iterrows():\n",
    "#     image_id = row['ImageID']\n",
    "#     question = row['Question']\n",
    "#     answer = row['Answer']\n",
    "    \n",
    "#     # Lấy category từ tên ảnh (phần trước dấu _)\n",
    "#     category = image_id.split('_')[0]\n",
    "    \n",
    "#     # Tạo đường dẫn ảnh\n",
    "#     image_path = os.path.join(image_folder, category, f\"{image_id}.png\")\n",
    "    \n",
    "#     # Kiểm tra ảnh tồn tại\n",
    "#     if os.path.exists(image_path):\n",
    "#         try:\n",
    "#             img = Image.open(image_path).convert('RGB')\n",
    "#             data.append({\n",
    "#                 'image': img,\n",
    "#                 'question': question,\n",
    "#                 'answer': answer\n",
    "#             })\n",
    "#         except:\n",
    "#             print(f\"Lỗi ảnh: {image_id}\")\n",
    "#     else:\n",
    "#         print(f\"Không tìm thấy: {image_path}\")\n",
    "\n",
    "# # Tạo dataset\n",
    "# dataset = Dataset.from_list(data)\n",
    "# print(f\"Dataset có {len(dataset)} mẫu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T20:04:28.558639Z",
     "iopub.status.busy": "2025-06-18T20:04:28.558040Z",
     "iopub.status.idle": "2025-06-18T20:04:28.568657Z",
     "shell.execute_reply": "2025-06-18T20:04:28.567922Z",
     "shell.execute_reply.started": "2025-06-18T20:04:28.558615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9, Test: 1\n"
     ]
    }
   ],
   "source": [
    "# # Chia train/test\n",
    "# train_test = dataset.train_test_split(test_size=0.1)\n",
    "# train_dataset = train_test['train']\n",
    "# test_dataset = train_test['test']\n",
    "\n",
    "# print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T20:04:29.605313Z",
     "iopub.status.busy": "2025-06-18T20:04:29.604555Z",
     "iopub.status.idle": "2025-06-18T20:04:30.688327Z",
     "shell.execute_reply": "2025-06-18T20:04:30.687584Z",
     "shell.execute_reply.started": "2025-06-18T20:04:29.605288Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97586207d64e45e299efcf45a296825c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba9e49df1994816a4b50e04fd255310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def preprocess(examples):\n",
    "#     \"\"\"Xử lý dữ liệu đầu vào cho training\"\"\"\n",
    "    \n",
    "#     batch_images = []\n",
    "#     batch_questions = []\n",
    "#     batch_answers = []\n",
    "    \n",
    "#     for i in range(len(examples['image'])):\n",
    "#         # Xử lý ảnh\n",
    "#         image = examples['image'][i]\n",
    "#         if isinstance(image, str):\n",
    "#             image = Image.open(image).convert('RGB')\n",
    "        \n",
    "#         # Thêm image token vào đầu câu hỏi\n",
    "#         question = f\"<image>{examples['question'][i]}\"\n",
    "#         answer = examples['answer'][i]\n",
    "        \n",
    "#         batch_images.append(image)\n",
    "#         batch_questions.append(question)\n",
    "#         batch_answers.append(answer)\n",
    "    \n",
    "#     # Tokenize inputs\n",
    "#     inputs = processor(\n",
    "#         text=batch_questions,\n",
    "#         images=batch_images,\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=256\n",
    "#     )\n",
    "    \n",
    "#     # Tokenize labels (answers)\n",
    "#     labels = processor.tokenizer(\n",
    "#         text=batch_answers,\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=64\n",
    "#     )\n",
    "    \n",
    "#     inputs[\"labels\"] = labels.input_ids\n",
    "    \n",
    "#     return inputs\n",
    "\n",
    "# # Áp dụng preprocessing\n",
    "# train_processed = train_dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)\n",
    "# test_processed = test_dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-18T19:26:28.290371Z",
     "iopub.status.busy": "2025-06-18T19:26:28.289683Z",
     "iopub.status.idle": "2025-06-18T19:26:30.403662Z",
     "shell.execute_reply": "2025-06-18T19:26:30.402646Z",
     "shell.execute_reply.started": "2025-06-18T19:26:28.290339Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5f5328bd154408bd8072291a31a6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9795140d6b4084b8a696722fdb9851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 3 named input_ids expected length 1 but got length 768",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/1027324431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Áp dụng preprocessing (cần có processor đã load)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtrain_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtest_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3077\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3079\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3080\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3538\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3539\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3540\u001b[0;31m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_original_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_original_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3541\u001b[0m                         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_examples_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPBAR_REFRESH_TIME_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0minferred_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inferred_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_schema\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 3 named input_ids expected length 1 but got length 768"
     ]
    }
   ],
   "source": [
    "# # Preprocessing cho PaliGemma\n",
    "# def preprocess(examples):\n",
    "#     # Thêm <image> token vào đầu question\n",
    "#     questions = [f\"<image>{q}\" for q in examples['question']]\n",
    "    \n",
    "#     # Process với processor\n",
    "#     inputs = processor(\n",
    "#         text=questions,\n",
    "#         images=examples['image'],\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512\n",
    "#     )\n",
    "    \n",
    "#     # Process answers\n",
    "#     labels = processor.tokenizer(\n",
    "#         text=examples['answer'],\n",
    "#         return_tensors=\"pt\", \n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=128\n",
    "#     ).input_ids\n",
    "    \n",
    "#     inputs[\"labels\"] = labels\n",
    "#     return {k: v.squeeze() for k, v in inputs.items()}\n",
    "\n",
    "# # Áp dụng preprocessing (cần có processor đã load)\n",
    "# train_processed = train_dataset.map(preprocess, batched=True)\n",
    "# test_processed = test_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T20:04:36.854390Z",
     "iopub.status.busy": "2025-06-18T20:04:36.853591Z",
     "iopub.status.idle": "2025-06-18T20:04:36.891085Z",
     "shell.execute_reply": "2025-06-18T20:04:36.890555Z",
     "shell.execute_reply.started": "2025-06-18T20:04:36.854359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Cấu hình training\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./paligemma-vietnamese-vqa\",\n",
    "#     per_device_train_batch_size=8,  # Giảm batch size do memory hạn chế\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     gradient_accumulation_steps=16,   # Tăng để compensate batch size nhỏ\n",
    "#     num_train_epochs=1,\n",
    "#     warmup_steps=100,\n",
    "#     logging_steps=10,\n",
    "#     save_steps=100,\n",
    "#     eval_steps=100,\n",
    "#     learning_rate=10e-4,\n",
    "#     weight_decay=0.01,\n",
    "#     fp16=True,  # Sử dụng mixed precision\n",
    "#     dataloader_pin_memory=False,\n",
    "#     remove_unused_columns=False,\n",
    "#     push_to_hub=False,\n",
    "#     report_to=None,\n",
    "#     gradient_checkpointing=True,  # Tiết kiệm memory\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Custom Data Collator\n",
    "# class VQADataCollator:\n",
    "#     def __init__(self, processor):\n",
    "#         self.processor = processor\n",
    "    \n",
    "#     def __call__(self, features):\n",
    "#         batch = {}\n",
    "        \n",
    "#         # Lấy keys từ feature đầu tiên\n",
    "#         keys = features[0].keys()\n",
    "        \n",
    "#         for key in keys:\n",
    "#             if key == \"labels\":\n",
    "#                 # Xử lý labels đặc biệt\n",
    "#                 labels = [f[key] for f in features]\n",
    "#                 batch[key] = torch.stack(labels)\n",
    "#             else:\n",
    "#                 # Stack các tensor khác\n",
    "#                 values = [f[key] for f in features]\n",
    "#                 if isinstance(values[0], torch.Tensor):\n",
    "#                     batch[key] = torch.stack(values)\n",
    "#                 else:\n",
    "#                     batch[key] = values\n",
    "        \n",
    "#         return batch\n",
    "\n",
    "# data_collator = VQADataCollator(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T20:04:39.902800Z",
     "iopub.status.busy": "2025-06-18T20:04:39.902130Z",
     "iopub.status.idle": "2025-06-18T20:06:44.599454Z",
     "shell.execute_reply": "2025-06-18T20:06:44.598608Z",
     "shell.execute_reply.started": "2025-06-18T20:04:39.902777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Bắt đầu fine-tuning PaliGemma...\n",
      "📊 Training với 9 samples\n",
      "🖥️  Device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Training bị dừng bởi người dùng\n"
     ]
    }
   ],
   "source": [
    "# # Fix cho các warning trong Trainer setup\n",
    "\n",
    "# import torch\n",
    "# import os\n",
    "# from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "# import logging\n",
    "\n",
    "# # 3. Custom callback để monitor\n",
    "# class DetailedTrainingCallback(TrainerCallback):\n",
    "#     def on_step_end(self, args, state, control, **kwargs):\n",
    "#         if state.global_step % 10 == 0:\n",
    "#             gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
    "#             print(f\"Step {state.global_step} | GPU Memory: {gpu_memory:.2f}GB\")\n",
    "            \n",
    "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "#         if logs:\n",
    "#             log_msg = \"Metrics: \"\n",
    "#             for key, value in logs.items():\n",
    "#                 if isinstance(value, float):\n",
    "#                     log_msg += f\"{key}: {value:.4f} | \"\n",
    "#             print(log_msg.rstrip(\" | \"))\n",
    "\n",
    "# # 4. Khởi tạo Trainer với cách mới (fix deprecation warning)\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     data_collator=data_collator,\n",
    "#     processing_class=processor,  # Thay vì tokenizer=processor.tokenizer\n",
    "#     callbacks=[DetailedTrainingCallback()],\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # 5. Thêm label names để tránh warning (nếu cần)\n",
    "# # Với VQA task, thường không cần label_names cụ thể\n",
    "# # nhưng có thể thêm nếu muốn tránh warning:\n",
    "# if hasattr(trainer.model.config, 'label_names') and trainer.model.config.label_names is None:\n",
    "#     trainer.label_names = []  # Hoặc danh sách labels cụ thể nếu có\n",
    "\n",
    "# print(\"🚀 Bắt đầu fine-tuning PaliGemma...\")\n",
    "# print(f\"📊 Training với {len(train_dataset)} samples\")\n",
    "# print(f\"🖥️  Device: {training_args.device}\")\n",
    "\n",
    "# # Bắt đầu training\n",
    "# try:\n",
    "#     trainer.train()\n",
    "    \n",
    "#     # Lưu model\n",
    "#     print(\"💾 Đang lưu model...\")\n",
    "#     trainer.save_model(\"./paligemma-vietnamese-vqa-final\")\n",
    "#     processor.save_pretrained(\"./paligemma-vietnamese-vqa-final\")\n",
    "#     print(\"✅ Hoàn thành!\")\n",
    "    \n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"⚠️ Training bị dừng bởi người dùng\")\n",
    "#     trainer.save_model(\"./paligemma-vietnamese-vqa-interrupted\")\n",
    "#     processor.save_pretrained(\"./paligemma-vietnamese-vqa-interrupted\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Lỗi: {str(e)}\")\n",
    "#     raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T19:12:44.789666Z",
     "iopub.status.busy": "2025-06-18T19:12:44.788980Z",
     "iopub.status.idle": "2025-06-18T19:12:45.498791Z",
     "shell.execute_reply": "2025-06-18T19:12:45.498184Z",
     "shell.execute_reply.started": "2025-06-18T19:12:44.789644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu hỏi: Màu của ảnh?\n",
      "Câu trả lời: yes\n"
     ]
    }
   ],
   "source": [
    "# def generate_answer(image_path, question, model, processor):\n",
    "#     \"\"\"Sinh câu trả lời cho câu hỏi về ảnh\"\"\"\n",
    "    \n",
    "#     # Tải và xử lý ảnh\n",
    "#     image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "#     # Chuẩn bị input\n",
    "#     prompt = f\"<image>{question}\"\n",
    "#     inputs = processor(\n",
    "#         text=prompt,\n",
    "#         images=image,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(model.device)\n",
    "    \n",
    "#     # Generate\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=100,\n",
    "#             do_sample=True,\n",
    "#             temperature=0.7,\n",
    "#             pad_token_id=processor.tokenizer.eos_token_id\n",
    "#         )\n",
    "    \n",
    "#     # Decode response\n",
    "#     response = processor.tokenizer.decode(\n",
    "#         outputs[0][inputs.input_ids.shape[1]:], \n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "    \n",
    "#     return response.strip()\n",
    "\n",
    "# # Test model\n",
    "# test_image = \"/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000001.png\"\n",
    "# test_question = \"Có bao nhiêu người trong ảnh này?\"\n",
    "\n",
    "# answer = generate_answer(test_image, test_question, model, processor)\n",
    "# print(f\"Câu hỏi: {test_question}\")\n",
    "# print(f\"Câu trả lời: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T10:04:58.490991Z",
     "iopub.status.busy": "2025-06-19T10:04:58.490664Z",
     "iopub.status.idle": "2025-06-19T10:04:58.508070Z",
     "shell.execute_reply": "2025-06-19T10:04:58.507481Z",
     "shell.execute_reply.started": "2025-06-19T10:04:58.490953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Optimize\n",
    "# # Giảm memory usage\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Sử dụng gradient checkpointing\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "# # Freeze một số layers nếu cần\n",
    "# for param in model.vision_tower.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def evaluate_vqa(model, processor, test_dataset):\n",
    "#     \"\"\"Đánh giá model trên test set\"\"\"\n",
    "    \n",
    "#     correct = 0\n",
    "#     total = len(test_dataset)\n",
    "    \n",
    "#     for item in test_dataset:\n",
    "#         predicted = generate_answer(\n",
    "#             item['image_path'], \n",
    "#             item['question'], \n",
    "#             model, \n",
    "#             processor\n",
    "#         )\n",
    "        \n",
    "#         # So sánh với ground truth\n",
    "#         if predicted.lower().strip() in item['answer'].lower().strip():\n",
    "#             correct += 1\n",
    "    \n",
    "#     accuracy = correct / total\n",
    "#     print(f\"Accuracy: {accuracy:.2%}\")\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-18T18:17:56.887046Z",
     "iopub.status.busy": "2025-06-18T18:17:56.886440Z",
     "iopub.status.idle": "2025-06-18T18:17:57.097681Z",
     "shell.execute_reply": "2025-06-18T18:17:57.096929Z",
     "shell.execute_reply.started": "2025-06-18T18:17:56.887026Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Education_000000000176</td>\n",
       "      <td>Các bạn học sinh đang làm gì trong lớp học?</td>\n",
       "      <td>Các bạn học sinh đang thực hiện các hoạt động ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Life_000000000565</td>\n",
       "      <td>Trong hình 1, bé gái đang chơi trò gì?</td>\n",
       "      <td>Bé gái đang chơi trò nhảy dây. Cô bé cầm dây m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life_000000000568</td>\n",
       "      <td>Khăn trải bàn có màu gì?</td>\n",
       "      <td>Khăn trải bàn có màu vàng kẻ ô vuông.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Life_000000000568</td>\n",
       "      <td>Trên bàn ăn có mấy người?</td>\n",
       "      <td>Trên bàn ăn có 4 người, gồm bố, mẹ, con trai v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Life_000000000784</td>\n",
       "      <td>Người đàn ông đang làm gì với chiếc điều khiển...</td>\n",
       "      <td>Người đàn ông đang sử dụng điều khiển từ xa để...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18833</th>\n",
       "      <td>Life_000000000550</td>\n",
       "      <td>Chuồng của những con vật được làm bằng gì?</td>\n",
       "      <td>Chuồng của những con vật được làm bằng lưới sắ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18834</th>\n",
       "      <td>Life_000000000551</td>\n",
       "      <td>Bé trai ở phía dưới bên trái đang làm gì?</td>\n",
       "      <td>Bé trai ở phía dưới bên trái đang hát.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18835</th>\n",
       "      <td>Life_000000000551</td>\n",
       "      <td>Có bao nhiêu con gà con trong ảnh?</td>\n",
       "      <td>Có 6 con gà con trong ảnh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18836</th>\n",
       "      <td>Life_000000000551</td>\n",
       "      <td>Người phụ nữ trong ảnh đang làm gì?</td>\n",
       "      <td>Người phụ nữ trong ảnh đang cho gà ăn.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18837</th>\n",
       "      <td>Life_000000000551</td>\n",
       "      <td>Hai bé gái ngồi trên bậc thềm đang nói gì?</td>\n",
       "      <td>Hai bé gái ngồi trên bậc thềm đang nói \"Con mè...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18838 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ImageID  \\\n",
       "0      Education_000000000176   \n",
       "1           Life_000000000565   \n",
       "2           Life_000000000568   \n",
       "3           Life_000000000568   \n",
       "4           Life_000000000784   \n",
       "...                       ...   \n",
       "18833       Life_000000000550   \n",
       "18834       Life_000000000551   \n",
       "18835       Life_000000000551   \n",
       "18836       Life_000000000551   \n",
       "18837       Life_000000000551   \n",
       "\n",
       "                                                Question  \\\n",
       "0            Các bạn học sinh đang làm gì trong lớp học?   \n",
       "1                 Trong hình 1, bé gái đang chơi trò gì?   \n",
       "2                               Khăn trải bàn có màu gì?   \n",
       "3                              Trên bàn ăn có mấy người?   \n",
       "4      Người đàn ông đang làm gì với chiếc điều khiển...   \n",
       "...                                                  ...   \n",
       "18833         Chuồng của những con vật được làm bằng gì?   \n",
       "18834          Bé trai ở phía dưới bên trái đang làm gì?   \n",
       "18835                 Có bao nhiêu con gà con trong ảnh?   \n",
       "18836                Người phụ nữ trong ảnh đang làm gì?   \n",
       "18837         Hai bé gái ngồi trên bậc thềm đang nói gì?   \n",
       "\n",
       "                                                  Answer  \n",
       "0      Các bạn học sinh đang thực hiện các hoạt động ...  \n",
       "1      Bé gái đang chơi trò nhảy dây. Cô bé cầm dây m...  \n",
       "2                Khăn trải bàn có màu vàng kẻ ô vuông.\\n  \n",
       "3      Trên bàn ăn có 4 người, gồm bố, mẹ, con trai v...  \n",
       "4      Người đàn ông đang sử dụng điều khiển từ xa để...  \n",
       "...                                                  ...  \n",
       "18833  Chuồng của những con vật được làm bằng lưới sắ...  \n",
       "18834             Bé trai ở phía dưới bên trái đang hát.  \n",
       "18835                         Có 6 con gà con trong ảnh.  \n",
       "18836           Người phụ nữ trong ảnh đang cho gà ăn.\\n  \n",
       "18837  Hai bé gái ngồi trên bậc thềm đang nói \"Con mè...  \n",
       "\n",
       "[18838 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import Dataset\n",
    "# import pandas as pd\n",
    "\n",
    "# data = pd.read_csv(\"/kaggle/input/vieduvqa/Verify_Convert_80.csv\")  # hoặc .json\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-18T17:55:03.212788Z",
     "iopub.status.busy": "2025-06-18T17:55:03.211904Z",
     "iopub.status.idle": "2025-06-18T17:55:03.221930Z",
     "shell.execute_reply": "2025-06-18T17:55:03.221284Z",
     "shell.execute_reply.started": "2025-06-18T17:55:03.212761Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Education_000000000176</td>\n",
       "      <td>Các bạn học sinh đang làm gì trong lớp học?</td>\n",
       "      <td>Các bạn học sinh đang thực hiện các hoạt động ...</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Edu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Life_000000000565</td>\n",
       "      <td>Trong hình 1, bé gái đang chơi trò gì?</td>\n",
       "      <td>Bé gái đang chơi trò nhảy dây. Cô bé cầm dây m...</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life_000000000568</td>\n",
       "      <td>Khăn trải bàn có màu gì?</td>\n",
       "      <td>Khăn trải bàn có màu vàng kẻ ô vuông.\\n</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Life_000000000568</td>\n",
       "      <td>Trên bàn ăn có mấy người?</td>\n",
       "      <td>Trên bàn ăn có 4 người, gồm bố, mẹ, con trai v...</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Life_000000000784</td>\n",
       "      <td>Người đàn ông đang làm gì với chiếc điều khiển...</td>\n",
       "      <td>Người đàn ông đang sử dụng điều khiển từ xa để...</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18833</th>\n",
       "      <td>Life_000000000550</td>\n",
       "      <td>Chuồng của những con vật được làm bằng gì?</td>\n",
       "      <td>Chuồng của những con vật được làm bằng lưới sắ...</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18834</th>\n",
       "      <td>Life_000000000551</td>\n",
       "      <td>Bé trai ở phía dưới bên trái đang làm gì?</td>\n",
       "      <td>Bé trai ở phía dưới bên trái đang hát.</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18835</th>\n",
       "      <td>Life_000000000551</td>\n",
       "      <td>Có bao nhiêu con gà con trong ảnh?</td>\n",
       "      <td>Có 6 con gà con trong ảnh.</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18836</th>\n",
       "      <td>Life_000000000551</td>\n",
       "      <td>Người phụ nữ trong ảnh đang làm gì?</td>\n",
       "      <td>Người phụ nữ trong ảnh đang cho gà ăn.\\n</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18837</th>\n",
       "      <td>Life_000000000551</td>\n",
       "      <td>Hai bé gái ngồi trên bậc thềm đang nói gì?</td>\n",
       "      <td>Hai bé gái ngồi trên bậc thềm đang nói \"Con mè...</td>\n",
       "      <td>/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18838 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ImageID  \\\n",
       "0      Education_000000000176   \n",
       "1           Life_000000000565   \n",
       "2           Life_000000000568   \n",
       "3           Life_000000000568   \n",
       "4           Life_000000000784   \n",
       "...                       ...   \n",
       "18833       Life_000000000550   \n",
       "18834       Life_000000000551   \n",
       "18835       Life_000000000551   \n",
       "18836       Life_000000000551   \n",
       "18837       Life_000000000551   \n",
       "\n",
       "                                                Question  \\\n",
       "0            Các bạn học sinh đang làm gì trong lớp học?   \n",
       "1                 Trong hình 1, bé gái đang chơi trò gì?   \n",
       "2                               Khăn trải bàn có màu gì?   \n",
       "3                              Trên bàn ăn có mấy người?   \n",
       "4      Người đàn ông đang làm gì với chiếc điều khiển...   \n",
       "...                                                  ...   \n",
       "18833         Chuồng của những con vật được làm bằng gì?   \n",
       "18834          Bé trai ở phía dưới bên trái đang làm gì?   \n",
       "18835                 Có bao nhiêu con gà con trong ảnh?   \n",
       "18836                Người phụ nữ trong ảnh đang làm gì?   \n",
       "18837         Hai bé gái ngồi trên bậc thềm đang nói gì?   \n",
       "\n",
       "                                                  Answer  \\\n",
       "0      Các bạn học sinh đang thực hiện các hoạt động ...   \n",
       "1      Bé gái đang chơi trò nhảy dây. Cô bé cầm dây m...   \n",
       "2                Khăn trải bàn có màu vàng kẻ ô vuông.\\n   \n",
       "3      Trên bàn ăn có 4 người, gồm bố, mẹ, con trai v...   \n",
       "4      Người đàn ông đang sử dụng điều khiển từ xa để...   \n",
       "...                                                  ...   \n",
       "18833  Chuồng của những con vật được làm bằng lưới sắ...   \n",
       "18834             Bé trai ở phía dưới bên trái đang hát.   \n",
       "18835                         Có 6 con gà con trong ảnh.   \n",
       "18836           Người phụ nữ trong ảnh đang cho gà ăn.\\n   \n",
       "18837  Hai bé gái ngồi trên bậc thềm đang nói \"Con mè...   \n",
       "\n",
       "                                              image_path  \n",
       "0      /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Edu...  \n",
       "1      /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...  \n",
       "2      /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...  \n",
       "3      /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...  \n",
       "4      /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...  \n",
       "...                                                  ...  \n",
       "18833  /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...  \n",
       "18834  /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...  \n",
       "18835  /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...  \n",
       "18836  /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...  \n",
       "18837  /kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Lif...  \n",
       "\n",
       "[18838 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-18T17:55:21.622176Z",
     "iopub.status.busy": "2025-06-18T17:55:21.621613Z",
     "iopub.status.idle": "2025-06-18T17:55:21.700549Z",
     "shell.execute_reply": "2025-06-18T17:55:21.699731Z",
     "shell.execute_reply.started": "2025-06-18T17:55:21.622155Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000176.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000565.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000568.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000568.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000784.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Nature/Nature_000000000008.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Nature/Nature_000000000250.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Nature/Nature_000000000371.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Nature/Nature_000000000398.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Nature/Nature_000000000401.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Nature/Nature_000000000401.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000048.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000097.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000108.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000115.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000115.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000139.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000139.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000145.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000158.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000163.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Object/Object_000000000174.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000004.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000011.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000022.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000031.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000038.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000049.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000049.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000049.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000049.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000049.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000052.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000052.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000101.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000101.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Others/Others_000000000101.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000340.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000847.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000847.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000848.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000888.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000890.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000919.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000932.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000983.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000001010.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000001016.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000001027.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000172.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000177.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000178.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000190.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000268.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000276.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000277.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000278.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000308.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000309.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000310.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000344.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000364.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000383.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000396.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000396.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000421.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000421.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000450.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000450.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000455.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000455.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000462.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000462.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000469.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000471.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000471.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000471.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000471.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000471.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000480.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000484.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000484.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000497.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000497.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000536.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000536.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000536.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000536.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000536.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Life/Life_000000000537.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000001.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000001.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000001.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000001.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000001.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000002.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000002.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000002.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000002.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000002.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000003.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000003.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000003.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000003.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000003.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000004.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000004.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000004.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000004.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000004.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000005.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000005.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000005.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000005.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000005.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000006.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000006.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000006.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000006.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000006.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000007.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000007.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000007.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000007.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000007.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000008.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000008.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000008.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000008.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000008.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000009.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000009.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000009.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000009.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000009.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000010.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000010.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000010.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000010.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000010.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000011.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000011.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000011.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000011.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000011.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000012.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000012.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000012.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000013.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000013.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000013.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000013.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000014.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000014.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000014.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000014.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000014.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000015.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000015.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000015.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000015.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000015.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000016.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000016.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000016.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000016.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000016.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000017.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000017.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000017.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000017.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000017.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000018.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000019.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000019.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000019.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000019.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000019.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000020.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000020.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000020.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000020.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000020.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000021.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000021.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000021.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000021.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000021.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000022.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000022.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000022.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000022.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000022.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000023.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000024.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000024.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000024.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000024.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000024.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000025.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000025.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000025.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000025.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000025.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000026.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000026.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000026.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000027.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000027.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000027.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000027.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000027.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000028.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000028.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000028.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000028.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000028.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000029.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000029.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000029.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000029.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000029.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000030.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000030.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000030.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000030.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000030.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000031.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000031.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000031.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000031.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000032.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000032.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000032.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000032.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000033.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000033.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000033.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000033.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000033.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000034.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000034.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000034.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000034.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000034.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000035.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000035.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000035.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000035.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000035.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000036.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000036.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000036.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000036.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000036.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000037.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000037.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000037.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000037.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000037.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000038.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000038.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000038.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000038.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000038.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000039.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000039.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000039.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000039.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000039.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000040.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000040.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000040.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000040.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000040.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000041.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000041.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000041.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000041.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000041.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000042.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000042.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000042.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000042.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000043.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000043.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000043.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000043.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000043.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000044.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000044.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000044.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000044.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000044.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000045.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000045.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000045.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000045.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000045.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000046.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000046.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000046.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000046.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000046.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000047.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000047.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000047.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000047.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000047.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000048.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000048.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000048.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000049.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000049.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000049.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000049.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000050.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000050.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000050.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000050.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000050.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000051.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000051.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000051.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000051.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000051.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000052.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000052.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000052.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000052.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000052.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000053.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000053.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000053.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000053.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000053.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000054.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000054.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000054.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000054.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000054.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000055.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000055.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000055.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000055.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000055.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000056.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000056.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000056.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000056.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000056.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000057.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000057.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000057.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000057.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000058.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000058.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000058.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000058.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000059.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000059.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000059.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000059.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000059.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000060.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000060.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000060.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000060.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000060.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000061.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000061.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000061.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000061.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000061.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000062.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000062.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000062.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000062.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000062.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000063.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000063.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000063.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000064.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000064.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000064.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000064.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000065.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000065.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000065.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000065.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000065.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000066.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000066.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000066.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000066.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000066.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000067.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000067.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000067.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000067.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000067.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000068.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000068.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000068.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000068.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000069.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000069.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000069.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000069.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000069.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000070.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000070.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000070.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000070.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000070.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000071.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000071.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000071.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000071.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000071.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000072.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000072.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000072.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000072.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000072.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000073.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000073.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000073.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000073.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000073.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000074.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000074.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000074.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000074.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000074.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000075.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000075.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000075.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000075.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000075.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000076.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000076.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000076.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000077.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000077.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000077.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000077.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000077.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000078.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000078.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000078.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000078.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000079.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000079.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000079.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000079.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000080.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000080.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000080.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000080.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000080.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000081.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000081.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000081.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000081.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000081.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000082.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000082.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000082.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000082.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000083.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000083.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000083.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000083.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000084.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000084.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000084.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000084.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000084.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000085.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000085.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000085.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000085.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000086.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000086.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000086.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000086.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000087.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000087.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000087.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000088.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000088.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000088.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000088.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000089.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000089.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000089.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000089.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000089.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000090.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000090.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000090.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000091.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000091.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000091.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000091.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000092.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000092.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000092.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000092.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000093.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000093.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000093.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000093.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000093.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000094.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000094.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000094.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000094.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000095.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000095.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000095.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000095.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000095.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000096.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000096.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000096.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000096.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000097.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000097.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000097.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000098.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000098.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000098.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000098.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000098.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000099.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000099.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000099.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000100.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000100.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000100.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000100.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000101.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000101.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000101.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000101.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000102.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000102.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000102.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000102.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000103.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000103.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000103.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000103.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000104.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000104.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000104.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000104.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000104.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000105.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000105.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000105.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000105.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000105.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000106.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000106.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000106.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000106.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000106.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000107.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000107.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000107.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000107.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000107.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000108.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000108.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000108.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000108.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000108.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000109.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000109.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000109.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000109.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000109.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000110.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000110.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000110.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000110.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000110.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000111.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000111.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000111.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000111.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000111.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000112.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000112.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000112.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000113.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000113.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000113.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000113.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000113.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000114.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000114.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000114.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000114.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000114.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000115.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000115.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000115.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000115.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000115.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000116.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000116.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000116.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000116.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000116.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000117.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000117.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000117.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000118.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000118.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000118.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000118.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000118.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000119.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000119.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000119.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000119.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000119.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000120.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000120.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000120.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000120.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000120.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000121.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000121.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000121.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000121.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000121.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000122.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000122.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000122.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000123.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000123.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000123.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000123.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000123.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000124.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000124.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000124.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000124.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000124.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000125.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000125.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000125.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000125.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000125.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000126.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000126.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000126.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000126.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000126.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000127.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000127.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000127.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000128.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000128.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000128.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000128.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000128.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000129.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000129.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000129.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000129.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000129.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000130.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000130.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000130.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000130.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000130.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000131.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000131.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000131.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000131.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000131.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000132.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000132.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000132.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000132.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000132.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000133.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000133.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000133.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000133.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000133.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000134.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000134.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000134.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000134.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000134.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000135.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000135.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000135.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000135.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000135.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000136.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000136.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000136.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000136.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000136.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000137.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000137.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000137.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000138.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000138.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000138.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000138.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000138.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000139.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000139.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000139.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000139.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000140.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000140.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000140.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000140.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000140.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000141.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000141.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000141.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000141.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000141.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000142.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000142.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000142.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000142.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000142.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000143.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000143.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000143.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000143.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000143.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000144.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000144.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000144.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000144.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000144.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000145.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000145.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000145.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000145.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000145.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000146.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000146.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000146.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000146.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000146.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000147.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000147.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000147.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000147.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000147.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000148.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000148.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000148.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000148.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000148.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000149.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000149.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000149.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000149.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000149.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000150.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000150.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000150.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000150.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000150.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000151.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000151.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000151.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000151.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000151.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000152.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000152.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000152.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000152.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000153.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000153.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000153.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000153.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000153.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000154.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000154.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000154.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000154.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000154.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000155.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000155.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000155.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000155.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000155.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000156.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000156.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000156.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000156.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000156.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000157.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000157.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000157.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000157.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000157.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000158.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000158.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000158.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000158.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000158.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000159.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000159.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000159.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000159.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000159.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000160.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000160.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000160.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000161.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000161.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000161.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000161.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000162.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000162.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000162.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000162.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000162.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000163.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000163.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000163.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000163.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000163.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000164.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000164.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000164.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000164.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000164.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000165.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000165.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000165.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000166.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000166.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000166.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000166.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000166.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000167.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000167.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000167.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000167.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000167.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000168.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000168.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000168.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000168.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000168.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000169.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000169.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000169.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000169.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000169.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000170.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000170.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000170.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000170.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000170.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000171.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000171.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000171.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000171.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000171.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000172.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000172.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000172.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000172.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000172.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000173.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000173.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000173.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000173.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000173.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000174.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000174.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000174.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000174.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000174.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000175.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000175.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000175.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000175.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000175.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000176.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000176.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000176.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000176.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000177.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000177.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000177.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000177.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000177.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000178.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000178.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000178.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000178.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000178.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000179.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000179.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000179.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000179.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000179.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000180.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000180.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000180.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000180.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000180.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000181.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000181.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000181.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000181.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000181.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000182.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000182.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000182.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000182.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000182.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000183.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000183.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000183.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000183.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000183.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000184.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000184.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000184.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000184.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000184.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000185.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000185.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000185.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000185.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000185.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000186.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000186.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000186.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000186.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000186.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000187.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000187.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000187.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000187.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000187.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000188.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000188.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000188.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000188.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000188.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000189.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000189.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000189.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000189.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000189.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000190.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000190.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000190.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000190.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000190.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000191.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000191.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000191.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000191.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000191.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000192.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000192.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000192.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000192.png',\n",
       " '/kaggle/input/vieduvqa/ViVQA4Edu/ViVQA4Edu/Education/Education_000000000192.png',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import Dataset\n",
    "\n",
    "# dataset = Dataset.from_pandas(data[[\"image_path\", \"Question\", \"Answer\"]])\n",
    "# dataset['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-18T18:06:10.211Z",
     "iopub.execute_input": "2025-06-18T18:01:09.105033Z",
     "iopub.status.busy": "2025-06-18T18:01:09.104484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e040cf001f4fc0b16b09243ccd9ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# def preprocess(ex):\n",
    "#     # Process inputs (image + text)\n",
    "#     # Optionally add image token manually to suppress warning\n",
    "#     question_with_image_token = f\"<image>{ex['Question']}\"\n",
    "    \n",
    "#     inputs = processor(\n",
    "#         text=question_with_image_token, \n",
    "#         images=Image.open(ex[\"image_path\"]).convert(\"RGB\"), \n",
    "#         return_tensors=\"pt\", \n",
    "#         padding=\"max_length\", \n",
    "#         truncation=True\n",
    "#     )\n",
    "    \n",
    "#     # Process labels (text only) - use the tokenizer directly\n",
    "#     labels = processor.tokenizer(\n",
    "#         text=ex[\"Answer\"], \n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=\"max_length\", \n",
    "#         truncation=True\n",
    "#     ).input_ids\n",
    "    \n",
    "#     # Add labels to inputs\n",
    "#     inputs[\"labels\"] = labels\n",
    "    \n",
    "#     # Squeeze batch dimension if needed\n",
    "#     return {k: v.squeeze() for k, v in inputs.items()}\n",
    "\n",
    "# dataset = dataset.map(preprocess)\n",
    "# train_ds, val_ds = ds.train_test_split(0.1).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-18T17:49:59.100517Z",
     "iopub.status.busy": "2025-06-18T17:49:59.100225Z",
     "iopub.status.idle": "2025-06-18T17:49:59.529214Z",
     "shell.execute_reply": "2025-06-18T17:49:59.528288Z",
     "shell.execute_reply.started": "2025-06-18T17:49:59.100496Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d8b2193e2d465ea16942c845c7d543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16954 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`images` are expected as arguments to a `PaliGemmaProcessor` instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/961325392.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3077\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3079\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3080\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3499\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3500\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3501\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3502\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3503\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36miter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3473\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3474\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3475\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3396\u001b[0m             \u001b[0;34m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3397\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3398\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3399\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/961325392.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(ex)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/paligemma/processing_paligemma.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`images` are expected as arguments to a `PaliGemmaProcessor` instance.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             logger.warning_once(\n",
      "\u001b[0;31mValueError\u001b[0m: `images` are expected as arguments to a `PaliGemmaProcessor` instance."
     ]
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# def preprocess(ex):\n",
    "#     img = Image.open(ex[\"image_path\"]).convert(\"RGB\")\n",
    "#     inp = processor(images=img, text=ex[\"Question\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "#     inp[\"labels\"] = processor(text=ex[\"Answer\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True).input_ids\n",
    "#     return {k: v.squeeze(0) for k, v in inp.items()}\n",
    "\n",
    "# train_ds = train_ds.map(preprocess)\n",
    "# val_ds = val_ds.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T17:44:40.824857Z",
     "iopub.status.busy": "2025-06-18T17:44:40.824577Z",
     "iopub.status.idle": "2025-06-18T17:44:41.022487Z",
     "shell.execute_reply": "2025-06-18T17:44:41.021681Z",
     "shell.execute_reply.started": "2025-06-18T17:44:40.824834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,336,192 || all params: 2,926,802,672 || trainable%: 0.1140\n"
     ]
    }
   ],
   "source": [
    "# from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"SEQ_2_SEQ_LM\"\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-18T17:51:19.590954Z",
     "iopub.status.busy": "2025-06-18T17:51:19.590235Z",
     "iopub.status.idle": "2025-06-18T17:51:20.115382Z",
     "shell.execute_reply": "2025-06-18T17:51:20.114411Z",
     "shell.execute_reply.started": "2025-06-18T17:51:19.590930Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/978775794.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature. The following columns have been ignored: [image_path, Question, pixel_values, Answer]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/978775794.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2272\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Currently training with a batch size of: {self._train_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m         \u001b[0;31m# Data loader and number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2274\u001b[0;31m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fsdp_xla_v2_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_spmd_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_datasets_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_unused_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_collator_with_removed_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msignature_columns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    940\u001b[0m                 \u001b[0;34m\"No columns in the dataset match the model's forward method signature. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;34mf\"The following columns have been ignored: [{', '.join(ignored_columns)}]. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [image_path, Question, pixel_values, Answer]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=5,\n",
    "#     learning_rate=2e-5,\n",
    "#     logging_steps=10,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     fp16=True,\n",
    "#     save_total_limit=1\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_ds,\n",
    "#     eval_dataset=val_ds,\n",
    "#     tokenizer=processor.tokenizer\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"pali_vqa_lora8bit\")\n",
    "# processor.save_pretrained(\"pali_vqa_lora8bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# image = Image.open(\"images/test.jpg\").convert(\"RGB\")\n",
    "# question = \"Bức tranh vẽ cảnh gì?\"\n",
    "\n",
    "# inputs = processor(images=image, text=question, return_tensors=\"pt\").to(\"cuda\")\n",
    "# output = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "# generated_answer = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(\"Answer:\", generated_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created simplified_dataset.csv with 250 rows.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # Read the CSV file\n",
    "# df = pd.read_csv(r'D:\\IT\\GITHUB\\FinalProject_DataLabeling\\benchmark_dataset.csv')\n",
    "\n",
    "# # Create a new dataframe with only the required columns\n",
    "# simplified_df = df[['ImageID', 'SuggestedQuestion', 'SuggestedAnswer']].copy()\n",
    "\n",
    "# # Rename the columns\n",
    "# simplified_df.rename(columns={\n",
    "#     'SuggestedQuestion': 'Question',\n",
    "#     'SuggestedAnswer': 'Answer'\n",
    "# }, inplace=True)\n",
    "\n",
    "# # Strip trailing newline characters from Answer field\n",
    "# simplified_df['Answer'] = simplified_df['Answer'].str.rstrip()\n",
    "\n",
    "# # Save the new dataframe to a CSV file\n",
    "# simplified_df.to_csv(r'D:\\IT\\GITHUB\\FinalProject_DataLabeling\\simplified_dataset.csv', index=False)\n",
    "\n",
    "# print(f\"Successfully created simplified_dataset.csv with {len(simplified_df)} rows.\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7690922,
     "sourceId": 12208768,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
